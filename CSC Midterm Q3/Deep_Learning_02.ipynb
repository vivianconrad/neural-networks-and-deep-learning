{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep_Learning_02.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ajjKcWJaZ-A9","outputId":"6271f4f7-4346-4801-bb3d-0ba525668c39"},"source":["# neural network with automatic validation set\n","from numpy import loadtxt\n","from keras.models import Sequential\n","from keras.layers import Dense\n","import numpy as np\n","\n","# load the dataset\n","data = loadtxt('diabetes.csv', delimiter=',')\n","# split dataset into input variables and output variables\n","input_x = data[:,0:8]\n","output_y = data[:,8]\n","\n","# define the keras model\n","model = Sequential()\n","\n","# fully connected layers are defined using the dense class\n","# we will use the relu activation function in the first two layers\n","\n","model.add(Dense(12, input_dim=8, activation='relu'))\n","model.add(Dense(8, activation='relu'))\n","\n","\n","# sigmoid activation function is used in the output layer.\n","# It ensure our network output is between 0 and 1\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# compile the keras model\n","# we use logarithmic loss, i.e., for binary classification problem in Keras is binary crossentropy.\n","# adam  is the efficient gradient decent algorithm used for optimization.\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Fit the model\n","model.fit(input_x, output_y, validation_split=0.33, epochs=150, batch_size=10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/150\n","35/52 [===================>..........] - ETA: 0s - loss: 6.2873 - accuracy: 0.4661 WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f9d104d7cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","52/52 [==============================] - 1s 6ms/step - loss: 5.8925 - accuracy: 0.4716 - val_loss: 3.6959 - val_accuracy: 0.5394\n","Epoch 2/150\n","52/52 [==============================] - 0s 2ms/step - loss: 3.7368 - accuracy: 0.4657 - val_loss: 2.7887 - val_accuracy: 0.5079\n","Epoch 3/150\n","52/52 [==============================] - 0s 2ms/step - loss: 3.0062 - accuracy: 0.4859 - val_loss: 2.3244 - val_accuracy: 0.5039\n","Epoch 4/150\n","52/52 [==============================] - 0s 2ms/step - loss: 2.0768 - accuracy: 0.5356 - val_loss: 1.7834 - val_accuracy: 0.6378\n","Epoch 5/150\n","52/52 [==============================] - 0s 2ms/step - loss: 1.8035 - accuracy: 0.5593 - val_loss: 1.4196 - val_accuracy: 0.5551\n","Epoch 6/150\n","52/52 [==============================] - 0s 2ms/step - loss: 1.2840 - accuracy: 0.6156 - val_loss: 1.1909 - val_accuracy: 0.6102\n","Epoch 7/150\n","52/52 [==============================] - 0s 2ms/step - loss: 1.3337 - accuracy: 0.5647 - val_loss: 1.1297 - val_accuracy: 0.6535\n","Epoch 8/150\n","52/52 [==============================] - 0s 2ms/step - loss: 1.2795 - accuracy: 0.5915 - val_loss: 1.0031 - val_accuracy: 0.6063\n","Epoch 9/150\n","52/52 [==============================] - 0s 2ms/step - loss: 1.1259 - accuracy: 0.5855 - val_loss: 0.9624 - val_accuracy: 0.6024\n","Epoch 10/150\n","52/52 [==============================] - 0s 2ms/step - loss: 1.0812 - accuracy: 0.5817 - val_loss: 0.9733 - val_accuracy: 0.5906\n","Epoch 11/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.9423 - accuracy: 0.5941 - val_loss: 0.8848 - val_accuracy: 0.6220\n","Epoch 12/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.8755 - accuracy: 0.6210 - val_loss: 0.9253 - val_accuracy: 0.5984\n","Epoch 13/150\n","52/52 [==============================] - 0s 2ms/step - loss: 1.0271 - accuracy: 0.5640 - val_loss: 0.8320 - val_accuracy: 0.6339\n","Epoch 14/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.8811 - accuracy: 0.6415 - val_loss: 0.8227 - val_accuracy: 0.6260\n","Epoch 15/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.8829 - accuracy: 0.5981 - val_loss: 0.8242 - val_accuracy: 0.6260\n","Epoch 16/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.7531 - accuracy: 0.7019 - val_loss: 0.8133 - val_accuracy: 0.6181\n","Epoch 17/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.8174 - accuracy: 0.6260 - val_loss: 0.7857 - val_accuracy: 0.6417\n","Epoch 18/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.7040 - accuracy: 0.6531 - val_loss: 0.7595 - val_accuracy: 0.6260\n","Epoch 19/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.7002 - accuracy: 0.6546 - val_loss: 0.7491 - val_accuracy: 0.6220\n","Epoch 20/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.7026 - accuracy: 0.6572 - val_loss: 0.8784 - val_accuracy: 0.6850\n","Epoch 21/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.7541 - accuracy: 0.6518 - val_loss: 0.7521 - val_accuracy: 0.6496\n","Epoch 22/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6729 - accuracy: 0.6505 - val_loss: 0.7451 - val_accuracy: 0.6142\n","Epoch 23/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.8111 - accuracy: 0.6231 - val_loss: 0.7261 - val_accuracy: 0.6378\n","Epoch 24/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6570 - accuracy: 0.6774 - val_loss: 0.7154 - val_accuracy: 0.6693\n","Epoch 25/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6255 - accuracy: 0.6564 - val_loss: 0.7186 - val_accuracy: 0.6299\n","Epoch 26/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6228 - accuracy: 0.6834 - val_loss: 0.7493 - val_accuracy: 0.6772\n","Epoch 27/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6540 - accuracy: 0.6538 - val_loss: 0.6940 - val_accuracy: 0.6693\n","Epoch 28/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.7060 - accuracy: 0.6529 - val_loss: 0.6881 - val_accuracy: 0.6378\n","Epoch 29/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6497 - accuracy: 0.6992 - val_loss: 0.6833 - val_accuracy: 0.6496\n","Epoch 30/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6283 - accuracy: 0.6959 - val_loss: 0.6744 - val_accuracy: 0.6654\n","Epoch 31/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6049 - accuracy: 0.7143 - val_loss: 0.6638 - val_accuracy: 0.6457\n","Epoch 32/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5846 - accuracy: 0.7026 - val_loss: 0.7257 - val_accuracy: 0.6457\n","Epoch 33/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6250 - accuracy: 0.6603 - val_loss: 0.7312 - val_accuracy: 0.6850\n","Epoch 34/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6674 - accuracy: 0.7016 - val_loss: 0.6728 - val_accuracy: 0.6575\n","Epoch 35/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6074 - accuracy: 0.7218 - val_loss: 0.6796 - val_accuracy: 0.6811\n","Epoch 36/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.7058 - accuracy: 0.6774 - val_loss: 0.6772 - val_accuracy: 0.6772\n","Epoch 37/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6556 - accuracy: 0.6981 - val_loss: 0.6680 - val_accuracy: 0.6811\n","Epoch 38/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.7220 - accuracy: 0.6598 - val_loss: 0.7042 - val_accuracy: 0.6181\n","Epoch 39/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.7171 - val_loss: 0.6442 - val_accuracy: 0.7008\n","Epoch 40/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6316 - accuracy: 0.6929 - val_loss: 0.6442 - val_accuracy: 0.6890\n","Epoch 41/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5986 - accuracy: 0.7060 - val_loss: 0.6371 - val_accuracy: 0.6969\n","Epoch 42/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5489 - accuracy: 0.7593 - val_loss: 0.7671 - val_accuracy: 0.6969\n","Epoch 43/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.7870 - accuracy: 0.6666 - val_loss: 0.6378 - val_accuracy: 0.6654\n","Epoch 44/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5628 - accuracy: 0.7319 - val_loss: 0.6368 - val_accuracy: 0.6929\n","Epoch 45/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6013 - accuracy: 0.7323 - val_loss: 0.6579 - val_accuracy: 0.6732\n","Epoch 46/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5643 - accuracy: 0.7109 - val_loss: 0.7329 - val_accuracy: 0.5669\n","Epoch 47/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6732 - accuracy: 0.6502 - val_loss: 0.6509 - val_accuracy: 0.6732\n","Epoch 48/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6299 - accuracy: 0.6521 - val_loss: 0.6336 - val_accuracy: 0.6614\n","Epoch 49/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6257 - accuracy: 0.7071 - val_loss: 0.8901 - val_accuracy: 0.5472\n","Epoch 50/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.7228 - accuracy: 0.6731 - val_loss: 0.6609 - val_accuracy: 0.6614\n","Epoch 51/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5812 - accuracy: 0.7044 - val_loss: 0.7254 - val_accuracy: 0.5906\n","Epoch 52/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6136 - accuracy: 0.7286 - val_loss: 0.6557 - val_accuracy: 0.6772\n","Epoch 53/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5479 - accuracy: 0.7502 - val_loss: 0.6365 - val_accuracy: 0.7283\n","Epoch 54/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5360 - accuracy: 0.7423 - val_loss: 0.6786 - val_accuracy: 0.6417\n","Epoch 55/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5655 - accuracy: 0.7149 - val_loss: 0.6143 - val_accuracy: 0.6850\n","Epoch 56/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5792 - accuracy: 0.7107 - val_loss: 0.7202 - val_accuracy: 0.6260\n","Epoch 57/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6277 - accuracy: 0.7066 - val_loss: 0.6779 - val_accuracy: 0.6811\n","Epoch 58/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6506 - accuracy: 0.6968 - val_loss: 0.6274 - val_accuracy: 0.6850\n","Epoch 59/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5631 - accuracy: 0.7096 - val_loss: 0.8378 - val_accuracy: 0.5433\n","Epoch 60/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5868 - accuracy: 0.7126 - val_loss: 0.6475 - val_accuracy: 0.6732\n","Epoch 61/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5309 - accuracy: 0.7746 - val_loss: 0.6180 - val_accuracy: 0.7087\n","Epoch 62/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5652 - accuracy: 0.7169 - val_loss: 0.9048 - val_accuracy: 0.6890\n","Epoch 63/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6713 - accuracy: 0.7006 - val_loss: 0.6294 - val_accuracy: 0.6732\n","Epoch 64/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5899 - accuracy: 0.7467 - val_loss: 0.8701 - val_accuracy: 0.7008\n","Epoch 65/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6170 - accuracy: 0.7275 - val_loss: 0.6043 - val_accuracy: 0.7165\n","Epoch 66/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5279 - accuracy: 0.7361 - val_loss: 0.6114 - val_accuracy: 0.7087\n","Epoch 67/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5883 - accuracy: 0.7170 - val_loss: 0.6766 - val_accuracy: 0.6024\n","Epoch 68/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5642 - accuracy: 0.7223 - val_loss: 0.6051 - val_accuracy: 0.7087\n","Epoch 69/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5582 - accuracy: 0.7311 - val_loss: 0.6260 - val_accuracy: 0.7047\n","Epoch 70/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.7778 - val_loss: 0.6116 - val_accuracy: 0.7047\n","Epoch 71/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5780 - accuracy: 0.7515 - val_loss: 0.7206 - val_accuracy: 0.6811\n","Epoch 72/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5847 - accuracy: 0.7168 - val_loss: 0.6287 - val_accuracy: 0.6693\n","Epoch 73/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5654 - accuracy: 0.7218 - val_loss: 0.6577 - val_accuracy: 0.6575\n","Epoch 74/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5882 - accuracy: 0.6866 - val_loss: 0.5986 - val_accuracy: 0.7323\n","Epoch 75/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5266 - accuracy: 0.7375 - val_loss: 0.6134 - val_accuracy: 0.7087\n","Epoch 76/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5192 - accuracy: 0.7505 - val_loss: 0.5899 - val_accuracy: 0.7402\n","Epoch 77/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5124 - accuracy: 0.7709 - val_loss: 0.6068 - val_accuracy: 0.7165\n","Epoch 78/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5199 - accuracy: 0.7592 - val_loss: 0.5886 - val_accuracy: 0.7323\n","Epoch 79/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5987 - accuracy: 0.7142 - val_loss: 0.6002 - val_accuracy: 0.7047\n","Epoch 80/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5954 - accuracy: 0.7065 - val_loss: 0.6998 - val_accuracy: 0.5906\n","Epoch 81/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5413 - accuracy: 0.7142 - val_loss: 0.5841 - val_accuracy: 0.7205\n","Epoch 82/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5228 - accuracy: 0.7450 - val_loss: 0.5830 - val_accuracy: 0.7244\n","Epoch 83/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5229 - accuracy: 0.7563 - val_loss: 0.5872 - val_accuracy: 0.7402\n","Epoch 84/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5046 - accuracy: 0.7663 - val_loss: 0.6386 - val_accuracy: 0.6969\n","Epoch 85/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5213 - accuracy: 0.7378 - val_loss: 0.5952 - val_accuracy: 0.7047\n","Epoch 86/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5963 - accuracy: 0.6983 - val_loss: 0.5857 - val_accuracy: 0.7362\n","Epoch 87/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5580 - accuracy: 0.7253 - val_loss: 0.5756 - val_accuracy: 0.7323\n","Epoch 88/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5118 - accuracy: 0.7605 - val_loss: 0.6085 - val_accuracy: 0.6811\n","Epoch 89/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5553 - accuracy: 0.7241 - val_loss: 0.5726 - val_accuracy: 0.7362\n","Epoch 90/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.7494 - val_loss: 0.6045 - val_accuracy: 0.7283\n","Epoch 91/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6005 - accuracy: 0.6880 - val_loss: 0.6047 - val_accuracy: 0.6969\n","Epoch 92/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5295 - accuracy: 0.7272 - val_loss: 0.5796 - val_accuracy: 0.7559\n","Epoch 93/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5524 - accuracy: 0.7166 - val_loss: 0.6102 - val_accuracy: 0.7244\n","Epoch 94/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6030 - accuracy: 0.7147 - val_loss: 0.6089 - val_accuracy: 0.6654\n","Epoch 95/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5286 - accuracy: 0.7763 - val_loss: 0.6536 - val_accuracy: 0.6969\n","Epoch 96/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5172 - accuracy: 0.7552 - val_loss: 0.5915 - val_accuracy: 0.7165\n","Epoch 97/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5621 - accuracy: 0.7349 - val_loss: 0.6322 - val_accuracy: 0.7165\n","Epoch 98/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7658 - val_loss: 0.5878 - val_accuracy: 0.7165\n","Epoch 99/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5625 - accuracy: 0.7089 - val_loss: 0.6520 - val_accuracy: 0.6850\n","Epoch 100/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6289 - accuracy: 0.6882 - val_loss: 0.5991 - val_accuracy: 0.7087\n","Epoch 101/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5897 - accuracy: 0.6960 - val_loss: 0.5894 - val_accuracy: 0.7244\n","Epoch 102/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.7825 - val_loss: 0.6084 - val_accuracy: 0.6732\n","Epoch 103/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5443 - accuracy: 0.6981 - val_loss: 0.6130 - val_accuracy: 0.6890\n","Epoch 104/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5284 - accuracy: 0.7543 - val_loss: 0.5622 - val_accuracy: 0.7520\n","Epoch 105/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5003 - accuracy: 0.7596 - val_loss: 0.5617 - val_accuracy: 0.7480\n","Epoch 106/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5170 - accuracy: 0.7252 - val_loss: 0.5669 - val_accuracy: 0.7323\n","Epoch 107/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5247 - accuracy: 0.7497 - val_loss: 0.6129 - val_accuracy: 0.7047\n","Epoch 108/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5172 - accuracy: 0.7427 - val_loss: 0.6037 - val_accuracy: 0.7008\n","Epoch 109/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5291 - accuracy: 0.7231 - val_loss: 0.5650 - val_accuracy: 0.7520\n","Epoch 110/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.7601 - val_loss: 0.6427 - val_accuracy: 0.6496\n","Epoch 111/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5470 - accuracy: 0.7364 - val_loss: 0.7043 - val_accuracy: 0.5866\n","Epoch 112/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5235 - accuracy: 0.7396 - val_loss: 0.5804 - val_accuracy: 0.7165\n","Epoch 113/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5437 - accuracy: 0.7289 - val_loss: 0.5621 - val_accuracy: 0.7520\n","Epoch 114/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5175 - accuracy: 0.7548 - val_loss: 0.5788 - val_accuracy: 0.7283\n","Epoch 115/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5839 - accuracy: 0.6936 - val_loss: 0.5545 - val_accuracy: 0.7638\n","Epoch 116/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4836 - accuracy: 0.7829 - val_loss: 0.5760 - val_accuracy: 0.7087\n","Epoch 117/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5226 - accuracy: 0.7607 - val_loss: 0.5794 - val_accuracy: 0.7283\n","Epoch 118/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5075 - accuracy: 0.7701 - val_loss: 0.6075 - val_accuracy: 0.7008\n","Epoch 119/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5282 - accuracy: 0.7298 - val_loss: 0.5713 - val_accuracy: 0.7283\n","Epoch 120/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5905 - accuracy: 0.7191 - val_loss: 0.5670 - val_accuracy: 0.7362\n","Epoch 121/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5083 - accuracy: 0.7503 - val_loss: 0.5719 - val_accuracy: 0.7165\n","Epoch 122/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7643 - val_loss: 0.5549 - val_accuracy: 0.7559\n","Epoch 123/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4918 - accuracy: 0.7659 - val_loss: 0.5716 - val_accuracy: 0.7323\n","Epoch 124/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.7750 - val_loss: 0.5769 - val_accuracy: 0.7126\n","Epoch 125/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5195 - accuracy: 0.7464 - val_loss: 0.6080 - val_accuracy: 0.7008\n","Epoch 126/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5599 - accuracy: 0.6896 - val_loss: 0.5671 - val_accuracy: 0.7362\n","Epoch 127/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4833 - accuracy: 0.7619 - val_loss: 0.5823 - val_accuracy: 0.7087\n","Epoch 128/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5273 - accuracy: 0.7401 - val_loss: 0.5531 - val_accuracy: 0.7677\n","Epoch 129/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4838 - accuracy: 0.7727 - val_loss: 0.5911 - val_accuracy: 0.7087\n","Epoch 130/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4924 - accuracy: 0.7552 - val_loss: 0.5605 - val_accuracy: 0.7598\n","Epoch 131/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4736 - accuracy: 0.7804 - val_loss: 0.5628 - val_accuracy: 0.7205\n","Epoch 132/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5071 - accuracy: 0.7655 - val_loss: 0.5750 - val_accuracy: 0.7126\n","Epoch 133/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5426 - accuracy: 0.7304 - val_loss: 0.5759 - val_accuracy: 0.7402\n","Epoch 134/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5136 - accuracy: 0.7545 - val_loss: 0.6008 - val_accuracy: 0.7008\n","Epoch 135/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5166 - accuracy: 0.7928 - val_loss: 0.5952 - val_accuracy: 0.6772\n","Epoch 136/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5138 - accuracy: 0.7294 - val_loss: 0.6328 - val_accuracy: 0.6969\n","Epoch 137/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5081 - accuracy: 0.7531 - val_loss: 0.6296 - val_accuracy: 0.6378\n","Epoch 138/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4997 - accuracy: 0.7721 - val_loss: 0.5811 - val_accuracy: 0.7165\n","Epoch 139/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4996 - accuracy: 0.7616 - val_loss: 0.5571 - val_accuracy: 0.7480\n","Epoch 140/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4755 - accuracy: 0.7679 - val_loss: 0.5620 - val_accuracy: 0.7323\n","Epoch 141/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5347 - accuracy: 0.7155 - val_loss: 0.5564 - val_accuracy: 0.7559\n","Epoch 142/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5087 - accuracy: 0.7469 - val_loss: 0.6644 - val_accuracy: 0.6220\n","Epoch 143/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.7487 - val_loss: 0.6488 - val_accuracy: 0.7008\n","Epoch 144/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4985 - accuracy: 0.7354 - val_loss: 0.6088 - val_accuracy: 0.7165\n","Epoch 145/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4912 - accuracy: 0.7745 - val_loss: 0.5485 - val_accuracy: 0.7598\n","Epoch 146/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5140 - accuracy: 0.7435 - val_loss: 0.5796 - val_accuracy: 0.7323\n","Epoch 147/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5291 - accuracy: 0.7528 - val_loss: 0.5859 - val_accuracy: 0.6929\n","Epoch 148/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5288 - accuracy: 0.7454 - val_loss: 0.5553 - val_accuracy: 0.7520\n","Epoch 149/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5380 - accuracy: 0.7308 - val_loss: 0.5544 - val_accuracy: 0.7441\n","Epoch 150/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4835 - accuracy: 0.7819 - val_loss: 0.5475 - val_accuracy: 0.7480\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f9d14b763d0>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wcXza80Yp4xD","outputId":"8d69c457-b6aa-4a94-a31d-5457b0b18a54"},"source":["# neural network with manual validation set\n","from numpy import loadtxt\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","# load the dataset\n","data = loadtxt('diabetes.csv', delimiter=',')\n","# split dataset into input variables and output variables\n","input_x = data[:,0:8]\n","output_y = data[:,8]\n","\n","# split into 67% for train and 33% for test\n","X_train, X_test, y_train, y_test = train_test_split(input_x, output_y, test_size=0.33)\n","\n","# create model\n","model = Sequential()\n","model.add(Dense(12, input_dim=8, activation='relu'))\n","model.add(Dense(8, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Fit the model\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=150, batch_size=10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/150\n","52/52 [==============================] - 1s 5ms/step - loss: 1.7910 - accuracy: 0.3872 - val_loss: 0.8526 - val_accuracy: 0.4921\n","Epoch 2/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.7707 - accuracy: 0.5365 - val_loss: 0.7597 - val_accuracy: 0.5591\n","Epoch 3/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.7056 - accuracy: 0.6112 - val_loss: 0.7274 - val_accuracy: 0.5709\n","Epoch 4/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6937 - accuracy: 0.6144 - val_loss: 0.7111 - val_accuracy: 0.6063\n","Epoch 5/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6743 - accuracy: 0.6887 - val_loss: 0.6999 - val_accuracy: 0.6220\n","Epoch 6/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6770 - accuracy: 0.6491 - val_loss: 0.6908 - val_accuracy: 0.6260\n","Epoch 7/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6556 - accuracy: 0.6973 - val_loss: 0.6767 - val_accuracy: 0.6535\n","Epoch 8/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6689 - accuracy: 0.6653 - val_loss: 0.6826 - val_accuracy: 0.6339\n","Epoch 9/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6557 - accuracy: 0.6503 - val_loss: 0.6691 - val_accuracy: 0.6614\n","Epoch 10/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6544 - accuracy: 0.6795 - val_loss: 0.6715 - val_accuracy: 0.6457\n","Epoch 11/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6538 - accuracy: 0.6524 - val_loss: 0.6647 - val_accuracy: 0.6614\n","Epoch 12/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6581 - accuracy: 0.6406 - val_loss: 0.6611 - val_accuracy: 0.6654\n","Epoch 13/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6434 - accuracy: 0.6438 - val_loss: 0.6589 - val_accuracy: 0.6654\n","Epoch 14/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6441 - accuracy: 0.6338 - val_loss: 0.6654 - val_accuracy: 0.6535\n","Epoch 15/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6461 - accuracy: 0.6564 - val_loss: 0.6601 - val_accuracy: 0.6535\n","Epoch 16/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6485 - accuracy: 0.6454 - val_loss: 0.6545 - val_accuracy: 0.6575\n","Epoch 17/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6444 - accuracy: 0.6348 - val_loss: 0.6567 - val_accuracy: 0.6575\n","Epoch 18/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6402 - accuracy: 0.6543 - val_loss: 0.6550 - val_accuracy: 0.6575\n","Epoch 19/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6396 - accuracy: 0.6583 - val_loss: 0.6491 - val_accuracy: 0.6693\n","Epoch 20/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6314 - accuracy: 0.6786 - val_loss: 0.6477 - val_accuracy: 0.6654\n","Epoch 21/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6191 - accuracy: 0.6778 - val_loss: 0.6470 - val_accuracy: 0.6693\n","Epoch 22/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6456 - accuracy: 0.6462 - val_loss: 0.6426 - val_accuracy: 0.6732\n","Epoch 23/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6398 - accuracy: 0.6495 - val_loss: 0.6458 - val_accuracy: 0.6693\n","Epoch 24/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6326 - accuracy: 0.6641 - val_loss: 0.6459 - val_accuracy: 0.6693\n","Epoch 25/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6511 - accuracy: 0.6196 - val_loss: 0.6406 - val_accuracy: 0.6811\n","Epoch 26/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6077 - accuracy: 0.6960 - val_loss: 0.6431 - val_accuracy: 0.6732\n","Epoch 27/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6207 - accuracy: 0.6828 - val_loss: 0.6282 - val_accuracy: 0.6772\n","Epoch 28/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6403 - accuracy: 0.6321 - val_loss: 0.6440 - val_accuracy: 0.6693\n","Epoch 29/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6127 - accuracy: 0.6774 - val_loss: 0.6355 - val_accuracy: 0.6732\n","Epoch 30/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6422 - accuracy: 0.6526 - val_loss: 0.6432 - val_accuracy: 0.6654\n","Epoch 31/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6383 - accuracy: 0.6571 - val_loss: 0.6391 - val_accuracy: 0.6732\n","Epoch 32/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6305 - accuracy: 0.6547 - val_loss: 0.6387 - val_accuracy: 0.6693\n","Epoch 33/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6190 - accuracy: 0.6760 - val_loss: 0.6297 - val_accuracy: 0.6732\n","Epoch 34/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6120 - accuracy: 0.6937 - val_loss: 0.6349 - val_accuracy: 0.6732\n","Epoch 35/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6315 - accuracy: 0.6590 - val_loss: 0.6346 - val_accuracy: 0.6693\n","Epoch 36/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6029 - accuracy: 0.7090 - val_loss: 0.6270 - val_accuracy: 0.6811\n","Epoch 37/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6093 - accuracy: 0.6934 - val_loss: 0.6169 - val_accuracy: 0.6890\n","Epoch 38/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6095 - accuracy: 0.7171 - val_loss: 0.6281 - val_accuracy: 0.6890\n","Epoch 39/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6248 - accuracy: 0.6646 - val_loss: 0.6164 - val_accuracy: 0.6929\n","Epoch 40/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6088 - accuracy: 0.6983 - val_loss: 0.6157 - val_accuracy: 0.6811\n","Epoch 41/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6012 - accuracy: 0.7123 - val_loss: 0.6120 - val_accuracy: 0.6969\n","Epoch 42/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5914 - accuracy: 0.6906 - val_loss: 0.6145 - val_accuracy: 0.7008\n","Epoch 43/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6035 - accuracy: 0.6715 - val_loss: 0.6179 - val_accuracy: 0.6811\n","Epoch 44/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5991 - accuracy: 0.7041 - val_loss: 0.6202 - val_accuracy: 0.6850\n","Epoch 45/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5946 - accuracy: 0.7019 - val_loss: 0.6097 - val_accuracy: 0.6850\n","Epoch 46/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6037 - accuracy: 0.6848 - val_loss: 0.6125 - val_accuracy: 0.6890\n","Epoch 47/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6304 - val_loss: 0.6056 - val_accuracy: 0.7047\n","Epoch 48/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5824 - accuracy: 0.7019 - val_loss: 0.6033 - val_accuracy: 0.6969\n","Epoch 49/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6052 - accuracy: 0.6843 - val_loss: 0.6079 - val_accuracy: 0.7087\n","Epoch 50/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6043 - accuracy: 0.6995 - val_loss: 0.6059 - val_accuracy: 0.7047\n","Epoch 51/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5760 - accuracy: 0.7175 - val_loss: 0.6081 - val_accuracy: 0.7126\n","Epoch 52/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5980 - accuracy: 0.6923 - val_loss: 0.6002 - val_accuracy: 0.6969\n","Epoch 53/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6027 - accuracy: 0.6852 - val_loss: 0.6030 - val_accuracy: 0.7008\n","Epoch 54/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6119 - accuracy: 0.6680 - val_loss: 0.6035 - val_accuracy: 0.7126\n","Epoch 55/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5959 - accuracy: 0.6937 - val_loss: 0.5987 - val_accuracy: 0.7087\n","Epoch 56/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5937 - accuracy: 0.6813 - val_loss: 0.5872 - val_accuracy: 0.7165\n","Epoch 57/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5863 - accuracy: 0.6887 - val_loss: 0.5987 - val_accuracy: 0.7087\n","Epoch 58/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5596 - accuracy: 0.7484 - val_loss: 0.5950 - val_accuracy: 0.7165\n","Epoch 59/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6148 - accuracy: 0.6815 - val_loss: 0.5974 - val_accuracy: 0.7008\n","Epoch 60/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5880 - accuracy: 0.7046 - val_loss: 0.5958 - val_accuracy: 0.7047\n","Epoch 61/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6088 - accuracy: 0.6793 - val_loss: 0.5974 - val_accuracy: 0.7047\n","Epoch 62/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5825 - accuracy: 0.7016 - val_loss: 0.6005 - val_accuracy: 0.7047\n","Epoch 63/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5729 - accuracy: 0.7191 - val_loss: 0.5866 - val_accuracy: 0.7126\n","Epoch 64/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5943 - accuracy: 0.6828 - val_loss: 0.5975 - val_accuracy: 0.7205\n","Epoch 65/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6178 - accuracy: 0.6701 - val_loss: 0.5923 - val_accuracy: 0.7047\n","Epoch 66/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6015 - accuracy: 0.6903 - val_loss: 0.5985 - val_accuracy: 0.6929\n","Epoch 67/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5858 - accuracy: 0.6951 - val_loss: 0.5778 - val_accuracy: 0.7165\n","Epoch 68/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5741 - accuracy: 0.7134 - val_loss: 0.5886 - val_accuracy: 0.7047\n","Epoch 69/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5635 - accuracy: 0.7226 - val_loss: 0.5828 - val_accuracy: 0.7165\n","Epoch 70/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5828 - accuracy: 0.7036 - val_loss: 0.5937 - val_accuracy: 0.6929\n","Epoch 71/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6043 - accuracy: 0.6795 - val_loss: 0.5996 - val_accuracy: 0.6969\n","Epoch 72/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5409 - accuracy: 0.7520 - val_loss: 0.5850 - val_accuracy: 0.7165\n","Epoch 73/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5911 - accuracy: 0.7057 - val_loss: 0.5911 - val_accuracy: 0.7323\n","Epoch 74/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5636 - accuracy: 0.7212 - val_loss: 0.5941 - val_accuracy: 0.7126\n","Epoch 75/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5685 - accuracy: 0.7255 - val_loss: 0.5810 - val_accuracy: 0.7205\n","Epoch 76/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5612 - accuracy: 0.7268 - val_loss: 0.5871 - val_accuracy: 0.7008\n","Epoch 77/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5941 - accuracy: 0.6979 - val_loss: 0.5857 - val_accuracy: 0.7047\n","Epoch 78/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5610 - accuracy: 0.7223 - val_loss: 0.5905 - val_accuracy: 0.7087\n","Epoch 79/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5711 - accuracy: 0.7218 - val_loss: 0.5752 - val_accuracy: 0.7244\n","Epoch 80/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6075 - accuracy: 0.6772 - val_loss: 0.5801 - val_accuracy: 0.7165\n","Epoch 81/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5689 - accuracy: 0.7040 - val_loss: 0.5757 - val_accuracy: 0.7205\n","Epoch 82/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5638 - accuracy: 0.7252 - val_loss: 0.5759 - val_accuracy: 0.7244\n","Epoch 83/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5755 - accuracy: 0.6808 - val_loss: 0.5939 - val_accuracy: 0.7008\n","Epoch 84/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5616 - accuracy: 0.7147 - val_loss: 0.5842 - val_accuracy: 0.7205\n","Epoch 85/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5634 - accuracy: 0.7177 - val_loss: 0.5812 - val_accuracy: 0.7087\n","Epoch 86/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5755 - accuracy: 0.7167 - val_loss: 0.5858 - val_accuracy: 0.7008\n","Epoch 87/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5662 - accuracy: 0.7119 - val_loss: 0.5739 - val_accuracy: 0.7283\n","Epoch 88/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5974 - accuracy: 0.6932 - val_loss: 0.5770 - val_accuracy: 0.7126\n","Epoch 89/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5680 - accuracy: 0.7068 - val_loss: 0.5700 - val_accuracy: 0.7402\n","Epoch 90/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5719 - accuracy: 0.6862 - val_loss: 0.5878 - val_accuracy: 0.7087\n","Epoch 91/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5518 - accuracy: 0.7352 - val_loss: 0.5810 - val_accuracy: 0.7047\n","Epoch 92/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5654 - accuracy: 0.7138 - val_loss: 0.5806 - val_accuracy: 0.7047\n","Epoch 93/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5685 - accuracy: 0.7175 - val_loss: 0.5697 - val_accuracy: 0.7244\n","Epoch 94/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5755 - accuracy: 0.7207 - val_loss: 0.5681 - val_accuracy: 0.7402\n","Epoch 95/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5716 - accuracy: 0.6969 - val_loss: 0.5825 - val_accuracy: 0.7126\n","Epoch 96/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5573 - accuracy: 0.7207 - val_loss: 0.5842 - val_accuracy: 0.7087\n","Epoch 97/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5598 - accuracy: 0.7177 - val_loss: 0.5719 - val_accuracy: 0.7402\n","Epoch 98/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5723 - accuracy: 0.7104 - val_loss: 0.5761 - val_accuracy: 0.7087\n","Epoch 99/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5555 - accuracy: 0.7326 - val_loss: 0.5616 - val_accuracy: 0.7441\n","Epoch 100/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5794 - accuracy: 0.7005 - val_loss: 0.6002 - val_accuracy: 0.6969\n","Epoch 101/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5717 - accuracy: 0.6950 - val_loss: 0.5707 - val_accuracy: 0.7283\n","Epoch 102/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5931 - accuracy: 0.6860 - val_loss: 0.5624 - val_accuracy: 0.7165\n","Epoch 103/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5595 - accuracy: 0.6966 - val_loss: 0.5691 - val_accuracy: 0.7244\n","Epoch 104/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5608 - accuracy: 0.7182 - val_loss: 0.5764 - val_accuracy: 0.7087\n","Epoch 105/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5651 - accuracy: 0.6958 - val_loss: 0.5806 - val_accuracy: 0.6850\n","Epoch 106/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5795 - accuracy: 0.6963 - val_loss: 0.5586 - val_accuracy: 0.7362\n","Epoch 107/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5575 - accuracy: 0.7194 - val_loss: 0.5630 - val_accuracy: 0.7244\n","Epoch 108/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5499 - accuracy: 0.6880 - val_loss: 0.5644 - val_accuracy: 0.7323\n","Epoch 109/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5831 - accuracy: 0.7022 - val_loss: 0.5630 - val_accuracy: 0.7205\n","Epoch 110/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5414 - accuracy: 0.7437 - val_loss: 0.5576 - val_accuracy: 0.7441\n","Epoch 111/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5488 - accuracy: 0.7037 - val_loss: 0.5557 - val_accuracy: 0.7520\n","Epoch 112/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5611 - accuracy: 0.7013 - val_loss: 0.5713 - val_accuracy: 0.7283\n","Epoch 113/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5589 - accuracy: 0.7228 - val_loss: 0.5592 - val_accuracy: 0.7323\n","Epoch 114/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5516 - accuracy: 0.7195 - val_loss: 0.5468 - val_accuracy: 0.7559\n","Epoch 115/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5414 - accuracy: 0.7133 - val_loss: 0.5596 - val_accuracy: 0.7283\n","Epoch 116/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5547 - accuracy: 0.7168 - val_loss: 0.5483 - val_accuracy: 0.7441\n","Epoch 117/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5617 - accuracy: 0.7027 - val_loss: 0.5545 - val_accuracy: 0.7205\n","Epoch 118/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5411 - accuracy: 0.7128 - val_loss: 0.5525 - val_accuracy: 0.7402\n","Epoch 119/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5311 - accuracy: 0.7528 - val_loss: 0.5544 - val_accuracy: 0.7283\n","Epoch 120/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5250 - accuracy: 0.7580 - val_loss: 0.5596 - val_accuracy: 0.7205\n","Epoch 121/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5511 - accuracy: 0.7269 - val_loss: 0.5517 - val_accuracy: 0.7402\n","Epoch 122/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5477 - accuracy: 0.7212 - val_loss: 0.5530 - val_accuracy: 0.7283\n","Epoch 123/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5594 - accuracy: 0.6756 - val_loss: 0.5570 - val_accuracy: 0.7165\n","Epoch 124/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5395 - accuracy: 0.7166 - val_loss: 0.5549 - val_accuracy: 0.7087\n","Epoch 125/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5363 - accuracy: 0.7379 - val_loss: 0.5522 - val_accuracy: 0.7244\n","Epoch 126/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5179 - accuracy: 0.7411 - val_loss: 0.5496 - val_accuracy: 0.7402\n","Epoch 127/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5441 - accuracy: 0.6966 - val_loss: 0.5409 - val_accuracy: 0.7402\n","Epoch 128/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7441 - val_loss: 0.5378 - val_accuracy: 0.7559\n","Epoch 129/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7084 - val_loss: 0.5443 - val_accuracy: 0.7402\n","Epoch 130/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5242 - accuracy: 0.7264 - val_loss: 0.5385 - val_accuracy: 0.7441\n","Epoch 131/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5368 - accuracy: 0.7117 - val_loss: 0.5433 - val_accuracy: 0.7362\n","Epoch 132/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5468 - accuracy: 0.7284 - val_loss: 0.5462 - val_accuracy: 0.7323\n","Epoch 133/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5251 - accuracy: 0.7364 - val_loss: 0.5626 - val_accuracy: 0.7283\n","Epoch 134/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5448 - accuracy: 0.7316 - val_loss: 0.5349 - val_accuracy: 0.7559\n","Epoch 135/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5299 - accuracy: 0.7215 - val_loss: 0.5485 - val_accuracy: 0.7126\n","Epoch 136/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5176 - accuracy: 0.7331 - val_loss: 0.5452 - val_accuracy: 0.7244\n","Epoch 137/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5257 - accuracy: 0.7298 - val_loss: 0.5353 - val_accuracy: 0.7480\n","Epoch 138/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5437 - accuracy: 0.7322 - val_loss: 0.5380 - val_accuracy: 0.7441\n","Epoch 139/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5459 - accuracy: 0.7057 - val_loss: 0.5294 - val_accuracy: 0.7559\n","Epoch 140/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5166 - accuracy: 0.7286 - val_loss: 0.5483 - val_accuracy: 0.7165\n","Epoch 141/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7349 - val_loss: 0.5448 - val_accuracy: 0.7244\n","Epoch 142/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5364 - accuracy: 0.7252 - val_loss: 0.5339 - val_accuracy: 0.7520\n","Epoch 143/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5538 - accuracy: 0.7079 - val_loss: 0.5478 - val_accuracy: 0.7441\n","Epoch 144/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5312 - accuracy: 0.7240 - val_loss: 0.5296 - val_accuracy: 0.7480\n","Epoch 145/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5244 - accuracy: 0.7556 - val_loss: 0.5386 - val_accuracy: 0.7362\n","Epoch 146/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5301 - accuracy: 0.7488 - val_loss: 0.5314 - val_accuracy: 0.7559\n","Epoch 147/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5007 - accuracy: 0.7591 - val_loss: 0.5365 - val_accuracy: 0.7362\n","Epoch 148/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.7364 - val_loss: 0.5324 - val_accuracy: 0.7480\n","Epoch 149/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5198 - accuracy: 0.7276 - val_loss: 0.5248 - val_accuracy: 0.7598\n","Epoch 150/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5234 - accuracy: 0.7362 - val_loss: 0.5310 - val_accuracy: 0.7520\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f9d0ec463d0>"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lcjvXPtIaFao","outputId":"755d3722-029a-4b11-e27e-ccb307311b0a"},"source":["# 10-fold cross validation\n","from numpy import loadtxt\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from sklearn.model_selection import StratifiedKFold\n","import numpy as np\n","\n","# load the dataset\n","data = loadtxt('diabetes.csv', delimiter=',')\n","\n","# split dataset into input variables and output variables\n","input_x = data[:,0:8]\n","output_y = data[:,8]\n","\n","# define 10-fold cross validation test harness\n","kfold = StratifiedKFold(n_splits=10, shuffle=True)\n","cross_validationores_scores = []\n","\n","for train, test in kfold.split(input_x, output_y):\n","  # create model\n","\tmodel = Sequential()\n","\tmodel.add(Dense(12, input_dim=8, activation='relu'))\n","\tmodel.add(Dense(8, activation='relu'))\n","\tmodel.add(Dense(1, activation='sigmoid'))\n"," \n","\t# Compile model\n","\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"," \n","\t# Fit the model\n","\tmodel.fit(input_x[train], output_y[train], epochs=150, batch_size=10, verbose=0)\n"," \n","\t# evaluate the model\n","\tscores = model.evaluate(input_x[test], output_y[test], verbose=0)\n"," \n","\tprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n","\tcross_validationores_scores.append(scores[1] * 100)\n"," \n","print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cross_validationores_scores), np.std(cross_validationores_scores)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["accuracy: 70.13%\n","accuracy: 63.64%\n","accuracy: 76.62%\n","WARNING:tensorflow:5 out of the last 3910 calls to <function Model.make_test_function.<locals>.test_function at 0x7f9d1282c440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","accuracy: 79.22%\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f9d116a9200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","accuracy: 66.23%\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f9d1583fd40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","accuracy: 70.13%\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f9d116a9320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","accuracy: 80.52%\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f9d12944a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","accuracy: 75.32%\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f9d12944440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","accuracy: 75.00%\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f9d14c059e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","accuracy: 75.00%\n","73.18% (+/- 5.21%)\n"],"name":"stdout"}]}]}