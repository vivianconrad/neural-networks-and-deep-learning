{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Human_Face_Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivianconrad/neural-networks-and-deep-learning/blob/main/Human_Face_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VB-LjRRYLh8",
        "outputId": "39ebd369-557f-414b-dafc-b2ca3aa244c0"
      },
      "source": [
        "# Human Face Recognition - 1 (Training)\n",
        "# Capturing frames from video and store them in human_faces folder\n",
        "\n",
        "!mkdir human_faces "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘human_faces’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqKr_DV1Z4vL"
      },
      "source": [
        "# Import Computer Vision package - cv2\n",
        "import cv2\n",
        "\n",
        "# Import Numerical Python package - numpy as np\n",
        "import numpy as np\n",
        "\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3bPHVfHYL50"
      },
      "source": [
        "# Load human face cascade file using cv2.CascadeClassifier built-in function\n",
        "# cv2.CascadeClassifier([filename]) \n",
        "face_detect = cv2.CascadeClassifier('haarcascade_frontalface.xml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "275WONpGYV0d"
      },
      "source": [
        "# Check if human face cascade file is loaded\n",
        "if face_detect.empty():\n",
        "\traise IOError('Unable to haarcascade_frontalface.xml file')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-42B0krwYbuf"
      },
      "source": [
        "# Defining face_detector function \n",
        "def face_detector(image):\n",
        "    # Function detects faces and returns the cropped face\n",
        "    # If no face detected, it returns the input image\n",
        "    \n",
        "    # Convert RGB to gray using cv2.COLOR_BGR2GRAY built-in function\n",
        "\t# BGR (bytes are reversed)\n",
        "\t# cv2.cvtColor: Converts image from one color space to another\n",
        "    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    # Detect objects(faces) of different sizes using cv2.CascadeClassifier.detectMultiScale\n",
        "    # cv2.CascadeClassifier.detectMultiScale(gray, scaleFactor, minNeighbors)\n",
        "   \n",
        "    # scaleFactor: Specifies the image size to be reduced\n",
        "    # Faces closer to the camera appear bigger than those faces in the back.\n",
        "    \n",
        "    # minNeighbors: Specifies the number of neighbors each rectangle should have to retain it\n",
        "    # Higher value results in less detections but with higher quality\n",
        "        \n",
        "    face_detection = face_detect.detectMultiScale(gray, 1.3, 5)\n",
        "    \n",
        "    if face_detection is ():\n",
        "        return None\n",
        "    \n",
        "    # Faces are cropped when they detected\n",
        "    for (x,y,w,h) in face_detection:\n",
        "        face_cropped = image[y:y+h, x:x+w]\n",
        "\n",
        "    return face_cropped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS_qj66DYfTq"
      },
      "source": [
        "# Initializing video capturing object\n",
        "capture = cv2.VideoCapture('video_1_vc.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzCYeV3pYh0x"
      },
      "source": [
        "# Initialize face_count to zero\n",
        "face_count = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsWpl7XPYoFv"
      },
      "source": [
        "# Initialize While Loop and execute until Esc key is pressed OR face_count == 200\n",
        "while True:\n",
        "\t# Start capturing frames\n",
        "\tret, capturing = capture.read()\n",
        "\t\n",
        "\tif face_detector(capturing) is not None:\n",
        "\t\tface_count += 1\n",
        "\t\t# Resize the frame using cv2.resize built-in function\n",
        "\t\t# cv2.resize(capturing, output image size, x scale, y scale, interpolation)\n",
        "\t\tresized_frame = cv2.resize(face_detector(capturing), (250, 250))\n",
        "\t\t\n",
        "\t\t# Convert RGB to gray using cv2.COLOR_BGR2GRAY built-in function\n",
        "\t\t# BGR (bytes are reversed)\n",
        "\t\t# cv2.cvtColor: Converts image from one color space to another\n",
        "\t\tgray = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Cropped faces are saved in human_faces folder\n",
        "        # Unique name is given to each cropped face \n",
        "\t\tpath = './human_faces/' + str(face_count) + '.jpg'\n",
        "\t\t\n",
        "\t\t# Save cropped faces in specified path using imwrite built-in function\n",
        "\t\tcv2.imwrite(path, gray)\n",
        "\t\t\n",
        "        \n",
        "        # Display face_count on cropped faces using cv2.putText\n",
        "\t\t#cv2.putText(image, string, orgin, font, fontScale, color, thickness)\n",
        "\t\tcv2.putText(gray, str(face_count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (255,255,0), 2)\n",
        "\t\t\n",
        "\t\t# Display cropped faces using imshow built-in function\n",
        "\t\tcv2_imshow(gray)\n",
        "        \n",
        "\telse:\n",
        "\t\tprint(\"Face NOT detected\")\n",
        "\t\tpass\n",
        "\n",
        "\tif cv2.waitKey(1) == 27 or face_count == 200: # 27 is the Esc Key\n",
        "\t\tbreak\n",
        "        \n",
        "# Close the capturing device\n",
        "capture.release()\n",
        "\n",
        "# Close all windows\n",
        "cv2.destroyAllWindows()\n",
        "   \n",
        "print(\"All cropped faces are saved in human_faces folder\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3UM-hDVvYpf"
      },
      "source": [
        "# Initializing video capturing object\n",
        "capture = cv2.VideoCapture('video_2_vc.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yjGa4_uvYyW"
      },
      "source": [
        "# Initialize face_count to zero\n",
        "face_count = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW_Ba8QrvTm5"
      },
      "source": [
        "# Initialize While Loop and execute until Esc key is pressed OR face_count == 200\n",
        "while True:\n",
        "\t# Start capturing frames\n",
        "\tret, capturing = capture.read()\n",
        "\t\n",
        "\tif face_detector(capturing) is not None:\n",
        "\t\tface_count += 1\n",
        "\t\t# Resize the frame using cv2.resize built-in function\n",
        "\t\t# cv2.resize(capturing, output image size, x scale, y scale, interpolation)\n",
        "\t\tresized_frame = cv2.resize(face_detector(capturing), (250, 250))\n",
        "\t\t\n",
        "\t\t# Convert RGB to gray using cv2.COLOR_BGR2GRAY built-in function\n",
        "\t\t# BGR (bytes are reversed)\n",
        "\t\t# cv2.cvtColor: Converts image from one color space to another\n",
        "\t\tgray = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Cropped faces are saved in human_faces folder\n",
        "        # Unique name is given to each cropped face \n",
        "\t\tpath = './human_faces/' + str(face_count) + '.jpg'\n",
        "\t\t\n",
        "\t\t# Save cropped faces in specified path using imwrite built-in function\n",
        "\t\tcv2.imwrite(path, gray)\n",
        "\t\t\n",
        "        \n",
        "        # Display face_count on cropped faces using cv2.putText\n",
        "\t\t#cv2.putText(image, string, orgin, font, fontScale, color, thickness)\n",
        "\t\tcv2.putText(gray, str(face_count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (255,255,0), 2)\n",
        "\t\t\n",
        "\t\t# Display cropped faces using imshow built-in function\n",
        "\t\tcv2_imshow(gray)\n",
        "        \n",
        "\telse:\n",
        "\t\tprint(\"Face NOT detected\")\n",
        "\t\tpass\n",
        "\n",
        "\tif cv2.waitKey(1) == 27 or face_count == 200: # 27 is the Esc Key\n",
        "\t\tbreak\n",
        "        \n",
        "# Close the capturing device\n",
        "capture.release()\n",
        "\n",
        "# Close all windows\n",
        "cv2.destroyAllWindows()\n",
        "   \n",
        "print(\"All cropped faces are saved in human_faces folder\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8meOfVC-YxSU"
      },
      "source": [
        "# Human Face Recognition - 2 (Testing)\n",
        "# Training using face images stored in human_faces folder\n",
        "# Testing using frames from the video\n",
        "\n",
        "# From Operating System(os) to return a list containing names\n",
        "# of the entries in the directory given by path - os.listdir(path)\n",
        "from os import listdir\n",
        " \n",
        "# os.path.isfile(path) - Returns True if path is an existing file\n",
        "from os.path import isfile, join"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojzpbcVNZBgo"
      },
      "source": [
        "# Face images for training are taken from human_faces folder\n",
        "path = './human_faces/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlF02PV6ZJHf"
      },
      "source": [
        "# To filter only files in the specified path we use:\n",
        "path_files = [f for f in listdir(path) if isfile(join(path, f))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5U8UizdZMld"
      },
      "source": [
        "# Two arrays are created, Training and Index(Label)\n",
        "Training, Index = [], []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-jRSuQmxhin"
      },
      "source": [
        "# Training images are opened from the path and\n",
        "# numpy array is created for training images \n",
        "\n",
        "for i, files in enumerate(path_files):\n",
        "\t#enumerate(path_files) loops over path_files & has automatic counter\n",
        "    \n",
        "    # Concatenate path and path_files in path_image variable\n",
        "    path_image = path + path_files[i]\n",
        "    \n",
        "    # Train images are read from path_image and converted to gray\n",
        "    train_images = cv2.imread(path_image, cv2.IMREAD_GRAYSCALE)\n",
        "    \n",
        "    # Convert train images into numpy array using np.asarray and\n",
        "    # append it with Training array \n",
        "    # Training.append(np.array(train_images, dtype)\n",
        "    # dtype=unit8 is an unsigned 8 bit integer (0 to 255)\n",
        "    Training.append(np.asarray(train_images, dtype=np.uint8))\n",
        "    \n",
        "    # Index array is appending for every i value\n",
        "    Index.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY0DpCNsZdRn"
      },
      "source": [
        "# Numpy array is created for Index using np.asarray\n",
        "# np.array(Index, dtype)\n",
        "# dtype=np.int32 is an 32 bit integer\n",
        "Index = np.asarray(Index, dtype=np.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgPHg5AsbeEF"
      },
      "source": [
        "# Local Binary Pattern Histogram (LBPH) is used for face recognition\n",
        "# LBP - For each pixel in grayscale image, neighborhood of size r \n",
        "# is selected surrounding the center pixel. LBP value is calculated\n",
        "# for this center pixel and stored in the output 2D array.\n",
        "\n",
        "# Histogram - Graphical representation of tonal distribution in image\n",
        "\n",
        "face_recognizer = cv2.face.LBPHFaceRecognizer_create() \n",
        "# OpenCV 3.0 use cv2.face.createLBPHFaceRecognizer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpqQxE_ObhkH",
        "outputId": "47032a7f-86fc-4336-fe65-bd4ab7d8ebe9"
      },
      "source": [
        "# Train the face_recognizer \n",
        "face_recognizer.train(np.asarray(Training), np.asarray(Index))\n",
        "print(\"Training completed successfully\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training completed successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC_EzoIObkSx"
      },
      "source": [
        "# Load human face cascade file using cv2.CascadeClassifier built-in function\n",
        "# cv2.CascadeClassifier([filename]) \n",
        "face_detect = cv2.CascadeClassifier('haarcascade_frontalface.xml')\n",
        "\n",
        "# Check if human face cascade file is loaded\n",
        "if face_detect.empty():\n",
        "\traise IOError('Unable to haarcascade_frontalfacet.xml file')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4OPciizZh__"
      },
      "source": [
        "# Defining face_detector function \n",
        "def face_detector(image, size=0.5):\n",
        "    \n",
        "    # Convert image to grayscale\n",
        "    # Convert RGB to gray using cv2.COLOR_BGR2GRAY built-in function\n",
        "\t# BGR (bytes are reversed)\n",
        "\t# cv2.cvtColor: Converts image from one color space to another\n",
        "    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "\t# Detect objects(faces) of different sizes using cv2.CascadeClassifier.detectMultiScale\n",
        "    # cv2.CascadeClassifier.detectMultiScale(gray, scaleFactor, minNeighbors)\n",
        "   \n",
        "    # scaleFactor: Specifies the image size to be reduced\n",
        "    # Faces closer to the camera appear bigger than those faces in the back.\n",
        "    \n",
        "    # minNeighbors: Specifies the number of neighbors each rectangle should have to retain it\n",
        "    # Higher value results in less detections but with higher quality\n",
        "        \n",
        "    face_detection = face_detect.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "    if face_detection is ():\n",
        "        return image, []\n",
        "    \n",
        "    for (x,y,w,h) in face_detection:\n",
        "\t\t# Rectangles are drawn around the face image using cv2.rectangle built-in function\n",
        "\t\t# cv2.rectangle(image, (x1,y1), (x2,y2), color, thickness)\n",
        "        cv2.rectangle(image,(x,y),(x+w,y+h),(0,0,255),4)\n",
        "        \n",
        "        # Crop the face within the rectangle\n",
        "        cropped = image[y:y+h, x:x+w]\n",
        "        \n",
        "        # Cropped face is resized to the same dimension as trained image (250 x 250)\n",
        "\t\t# cv2.resize(capturing, output image size, x scale, y scale, interpolation)\n",
        "        cropped = cv2.resize(cropped, (250, 250))\n",
        "    \n",
        "    return image, cropped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZFk9S6pZmSN"
      },
      "source": [
        "capture = cv2.VideoCapture('video_5_vc.mp4')\n",
        "# load the video for testing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jDvYQiD4ZpC5"
      },
      "source": [
        "# Initialize While Loop and execute until Esc key is pressed\n",
        "while True:\n",
        "    # Start capturing frames\n",
        "\tret, capturing = capture.read()\n",
        "    \n",
        "    # Call the function face_detector\n",
        "\timage, faces = face_detector(capturing)\n",
        "    \n",
        "\ttry:\n",
        "\t\t# Convert RGB to gray using cv2.COLOR_BGR2GRAY built-in function\n",
        "\t\t# BGR (bytes are reversed)\n",
        "\t\t# cv2.cvtColor: Converts image from one color space to another\n",
        "\t\tfaces = cv2.cvtColor(faces, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Faces is passed to the prediction model\n",
        "\t\tmatching = face_recognizer.predict(faces)\n",
        "        # matching tuple contains the index and the score (confidence) value \n",
        "        \n",
        "\t\tif matching[1] < 500:\n",
        "\t\t\tscore = int( 100 * (1 - (matching[1])/350) )\n",
        "\t\t\tstring = str(score) + '% Matching Confidence'\n",
        "        \n",
        "\t\tif score > 60:\n",
        "\t\t\t# Input the text string using cv2.putText\n",
        "\t\t\t#cv2.putText(image, string, orgin, font, fontScale, color, thickness)\n",
        "\t\t\tcv2.putText(image, string, (100, 100), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,255), 2)\n",
        "\t\t\tcv2.putText(image, \"Welcome Vi\", (210, 40), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,255), 2)\n",
        "\t\t\t\n",
        "\t\t\t# Display Real-time Face Recognition using imshow built-in function\n",
        "\t\t\tcv2_imshow(image)\n",
        "\t\t\n",
        "\t\telse:\n",
        "\t\t\tcv2.putText(image, \"This is NOT Vi. Get OUT.\", (150, 100), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 3)\n",
        "\t\t\tcv2_imshow(image)\n",
        "\n",
        "\texcept:\n",
        "\t\tcv2.putText(image, \"FACE NOT FOUND \", (150, 250) , cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 3)\n",
        "\t\tcv2_imshow(image)\n",
        "\t\tpass\n",
        "        \n",
        "\tc = cv2.waitKey(1)\n",
        "\tif c == 27:\n",
        "\t\tbreak\n",
        "        \n",
        "# Close the capturing device\n",
        "capture.release()\n",
        "\n",
        "# Close all windows\n",
        "cv2.destroyAllWindows()    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}