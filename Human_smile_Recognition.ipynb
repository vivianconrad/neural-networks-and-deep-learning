{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Human_smile_Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivianconrad/neural-networks-and-deep-learning/blob/main/Human_smile_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84rkBQzF4dea"
      },
      "source": [
        "#import packages\n",
        "import numpy as np\n",
        "import cv2\n",
        "from keras.preprocessing import image\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "from keras.models import model_from_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0TuIkpLd4ag"
      },
      "source": [
        "# Load human face cascade file using cv2.CascadeClassifier built-in function\n",
        "# cv2.CascadeClassifier([filename]) \n",
        "face_cascade_classifier = cv2.CascadeClassifier('haarcascade_frontalface.xml')\n",
        "smile_cascade_classifier = cv2.CascadeClassifier('haarcascade_smile.xml')\n",
        "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
        "#nose_detect = cv2.CascadeClassifier('haarcascade_mcs_nose.xml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxVsVxyqpMsU"
      },
      "source": [
        "if face_cascade_classifier.empty():\n",
        "  raise IOError('Unable to haarcascade_frontalface_alt.xml file')\n",
        "\n",
        "  if smile_cascade_classifier.empty():\n",
        "    raise IOError('Unable to haarcascade_smile_alt.xml file')\n",
        "\n",
        "  if eye_cascade.empty():\n",
        "    raise IOError('Unable to haarcascade_eye_alt.xml file')\n",
        "\n",
        "  #if nose_detect.empty():\n",
        "  #raise IOError('Unable to haarcascade_mcs_nose_alt.xml file')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TShVwZn9i_H2"
      },
      "source": [
        "# model_from_json : Parses a JSON model configuration string and returns a model instance\n",
        "face_model = model_from_json(open(\"facial_expression.json\", \"r\").read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ERtfr93jB8m"
      },
      "source": [
        "# load the face expression trained model\n",
        "face_model.load_weights('facial_expression.h5') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17mkk87NjEoa"
      },
      "source": [
        "# seven standard expressions\n",
        "expressions = ('Angry:', 'Disgust:', 'Fear:', 'Happy:', 'Sad:', 'Surprise:', 'Neutral:')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahgdK2OYjGKx"
      },
      "source": [
        "# load the video for facial expression recognition\n",
        "video = cv2.VideoCapture('video_4_vc.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtXDinR_ixrh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "5b26f634-d7d8-4f96-c413-484bb3c7eacb"
      },
      "source": [
        "frame = 0\n",
        "\n",
        "while(True):\n",
        "\n",
        "\tret, img = video.read()\n",
        "  \n",
        "  # Resize the frame using cv2.resize built-in function\n",
        "\t# cv2.resize(capturing, output image size, x scale, y scale, interpolation)\n",
        "\t#image = cv2.resize(image, (640, 360))\n",
        " \n",
        "\t#image = image[0:1200,:]\n",
        "\n",
        "  # Convert RGB to gray using cv2.COLOR_BGR2GRAY built-in function\n",
        "\t# BGR (bytes are reversed)\n",
        "\t# cv2.cvtColor: Converts image from one color space to another\n",
        "\tgray = cv2.cvtColor(resize_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Detect objects(faces) of different sizes using cv2.CascadeClassifier.detectMultiScale\n",
        "\t#cv2.CascadeClassifier.detectMultiScale(gray, scaleFactor, minNeighbors)\n",
        "   \n",
        "  # scaleFactor: Specifies the image size to be reduced\n",
        "  # Faces closer to the camera appear bigger than those faces in the back.\n",
        "    \n",
        "  # minNeighbors: Specifies the number of neighbors each rectangle should have to retain it\n",
        "  # Higher value results in less detections but with higher quality\n",
        "\ttry:\n",
        "\t\tresize_frame = cv2.resize(img, None, interpolation=cv2.INTER_AREA)\n",
        "\texcept:\n",
        "\t\tbreak\n",
        "\tgray = cv2.cvtColor(resize_frame, cv2.COLOR_BGR2GRAY)\n",
        "\tface_detection = face_cascade_classifier.detectMultiScale(gray, 1.3, 5)\n",
        " \n",
        "\tdef detect(gray, frame):\n",
        "\t#faces = face_cascade_classifier.detectMultiScale(gray, 1.3, 5)\n",
        "\t\tfor (x, y, w, h) in face_detection:\n",
        "\t\t\tcv2.rectangle(resize_frame, (x, y), ((x + w), (y + h)), (0, 0, 255), 10)\n",
        "\t\troi_gray = gray[y:y + h, x:x + w]\n",
        "\t\troi_color = frame[y:y + h, x:x + w]\n",
        "\t\tsmiles = smile_cascade_classifier.detectMultiScale(roi_gray, 1.8, 20)\n",
        "\t\teye_detector = eye_cascade.detectMultiScale(gray_roi)\n",
        "\t\t\n",
        "\t\tfor (eye_x, eye_y, eye_w, eye_h) in eye_detector:\n",
        "\t\t\tcv2.rectangle(color_roi,(eye_x,eye_y),((eye_x+ eye_w), (eye_y + eye_h),(255,0,0),5))\n",
        "  \n",
        "\t\tnose_detector = nose_detect.detectMultiScale(gray_roi, 1.3, 5)\n",
        "\t\tfor (nose_x, nose_y, nose_w, nose_h) in nose_detector:\n",
        "\t\t\tcv2.rectangle(color_roi, (nose_x, nose_y), (nose_x + nose_w, nose_y + nose_h), (0,255,0), 5)\n",
        "\n",
        "\t\tsmile_detector = smile_cascade_classifier.detectMultiScale(gray_roi, 1.3, 5)\n",
        "\t\tfor (smile_x, smile_y, smile_w, smile_h) in smile_detector:\n",
        "\t\t\tcv2.rectangle(roi_color, (sx, sy), ((sx + sw), (sy + sh)), (0, 0, 255), 2)\n",
        "   \n",
        "\t\tfor (x,y,w,h) in faces:\n",
        "\t\t\tfaces = f_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\t\tfor (x,y,w,h) in faces:\n",
        "\t\t\timg = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "\t\t\troi_gray = gray[y:y+h, x:x+w]\n",
        "\t\t\troi_color = img[y:y+h, x:x+w]\n",
        "\t\t\tsmile = e_cascade.detectMultiScale(roi_gray)\n",
        "\n",
        "\t\tif w > 130: #ignore small faces\n",
        "            \n",
        "\t\t\tcv2.rectangle(img,(x,y),(x+w,y+h),(64,64,64),2)\n",
        "\t\t\t\n",
        "\t\t\tface_detected = img[int(y):int(y+h), int(x):int(x+w)] \n",
        "\n",
        "\t\t\tface_detected = cv2.cvtColor(face_detected, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "\t\t\tface_detected = cv2.resize(face_detected, (48, 48))\n",
        "\t\t\t\n",
        "\t\t\timg_pixels = image.img_to_array(face_detected)\n",
        "   \n",
        "\t\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\n",
        "   \n",
        "\t\t\t#pixels are in scale of [0, 255]. normalize all pixels in scale of [0, 1]\n",
        "\t\t\timg_pixels /= 255 \n",
        "\n",
        "      #the probabilities of 7 expressions is stored in predictions\n",
        "\t\t\tpredictions = face_model.predict(img_pixels) \n",
        "\n",
        "\t\t\tmax_index = np.argmax(predictions[0])\n",
        "\t\t\t\n",
        "\t\t\t# copy the image in overlay\n",
        "\t\t\toverlay = img.copy()\n",
        "   \n",
        "      # higher value of opacity, the background will be of lower color\n",
        "\t\t\topacity = 0.2\n",
        "\n",
        "\t\t\tcv2.rectangle(img,(x+w+10,y-25),(x+w+150,y+115),(64,64,64),cv2.FILLED)\n",
        "   \n",
        "\t\t\tcv2.addWeighted(overlay, opacity, img, 1 - opacity, 0, img)\n",
        "\t\t\t\n",
        "\t\t\t# face and expressions are connected\n",
        "\t\t\tcv2.line(img,(int((x+x+w)/2),y+15),(x+w,y-20),(255,255,255),1)\n",
        "   \n",
        "\t\t\tcv2.line(img,(x+w,y-20),(x+w+10,y-20),(255,255,255),1)\n",
        "\t\t\t\n",
        "\t\t\texpression = \"\"\n",
        "\t\t\tfor i in range(len(predictions[0])):\n",
        "     \n",
        "\t\t\t\texpression = \"%s %s%s\" % (expressions[i], round(predictions[0][i]*100, 2), '%')\n",
        "\t\t\t\t\n",
        "\t\t\t\t\"\"\"if i != max_index:\n",
        "\t\t\t\t\tcolor = (255,0,0)\"\"\"\n",
        "\t\t\t\t\t\n",
        "\t\t\t\tcolor = (0,255,255)\n",
        "        \n",
        "\t\t\t\tcv2.putText(img, expression, (int(x+w+15), int(y-12+i*20)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "\t\n",
        "\tcv2_imshow(img)\n",
        "\t\n",
        "\tframe = frame + 1\n",
        "\n",
        "\tif frame > 227:\n",
        "\t\tbreak\n",
        "\t\n",
        "\tif cv2.waitKey(1) & 0xFF == ord('q'): #press q to quit\n",
        "\t\tbreak\n",
        "\n",
        "# Close the capturing device\n",
        "video.release()\n",
        "\n",
        "# Close all windows\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7106f25beb9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# BGR (bytes are reversed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# cv2.cvtColor: Converts image from one color space to another\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresize_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m# Detect objects(faces) of different sizes using cv2.CascadeClassifier.detectMultiScale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'resize_frame' is not defined"
          ]
        }
      ]
    }
  ]
}