{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Human_Facial_Emotion_Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivianconrad/neural-networks-and-deep-learning/blob/main/Human_Facial_Emotion_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84rkBQzF4dea"
      },
      "source": [
        "#import packages\n",
        "import numpy as np\n",
        "import cv2\n",
        "from keras.preprocessing import image\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "from keras.models import model_from_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0TuIkpLd4ag"
      },
      "source": [
        "# Load human face cascade file using cv2.CascadeClassifier built-in function\n",
        "# cv2.CascadeClassifier([filename]) \n",
        "face_cascade_classifier = cv2.CascadeClassifier('haarcascade_frontalface.xml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TShVwZn9i_H2"
      },
      "source": [
        "# model_from_json : Parses a JSON model configuration string and returns a model instance\n",
        "face_model = model_from_json(open(\"facial_expression.json\", \"r\").read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ERtfr93jB8m"
      },
      "source": [
        "# load the face expression trained model\n",
        "face_model.load_weights('facial_expression.h5') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17mkk87NjEoa"
      },
      "source": [
        "# seven standard expressions\n",
        "expressions = ('Angry:', 'Disgust:', 'Fear:', 'Happy:', 'Sad:', 'Surprise:', 'Neutral:')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahgdK2OYjGKx"
      },
      "source": [
        "# load the video for facial expression recognition\n",
        "video = cv2.VideoCapture('video_4_vc.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AtXDinR_ixrh",
        "outputId": "dd2b3834-e395-4d58-b073-8762361d9705"
      },
      "source": [
        "frame = 0\n",
        "\n",
        "while(True):\n",
        "\n",
        "\tret, img = video.read()\n",
        "  \n",
        "  # Resize the frame using cv2.resize built-in function\n",
        "\t# cv2.resize(capturing, output image size, x scale, y scale, interpolation)\n",
        "\timg = cv2.resize(img, (640, 360))\n",
        " \n",
        "\timg = img[0:1200,:]\n",
        "\n",
        "  # Convert RGB to gray using cv2.COLOR_BGR2GRAY built-in function\n",
        "\t# BGR (bytes are reversed)\n",
        "\t# cv2.cvtColor: Converts image from one color space to another\n",
        "\tgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Detect objects(faces) of different sizes using cv2.CascadeClassifier.detectMultiScale\n",
        "  # cv2.CascadeClassifier.detectMultiScale(gray, scaleFactor, minNeighbors)\n",
        "   \n",
        "  # scaleFactor: Specifies the image size to be reduced\n",
        "  # Faces closer to the camera appear bigger than those faces in the back.\n",
        "    \n",
        "  # minNeighbors: Specifies the number of neighbors each rectangle should have to retain it\n",
        "  # Higher value results in less detections but with higher quality\n",
        "\tfaces = face_cascade_classifier.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "\tfor (x,y,w,h) in faces:\n",
        "\n",
        "\t\tif w > 130: #ignore small faces\n",
        "\n",
        "\t\t\t#cv2.rectangle(img,(x,y),(x+w,y+h),(64,64,64),2)\n",
        "\t\t\t\n",
        "\t\t\tface_detected = img[int(y):int(y+h), int(x):int(x+w)] \n",
        "\n",
        "\t\t\tface_detected = cv2.cvtColor(face_detected, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "\t\t\tface_detected = cv2.resize(face_detected, (48, 48))\n",
        "\t\t\t\n",
        "\t\t\timg_pixels = image.img_to_array(face_detected)\n",
        "   \n",
        "\t\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\n",
        "   \n",
        "\t\t\t#pixels are in scale of [0, 255]. normalize all pixels in scale of [0, 1]\n",
        "\t\t\timg_pixels /= 255 \n",
        "\n",
        "      #the probabilities of 7 expressions is stored in predictions\n",
        "\t\t\tpredictions = face_model.predict(img_pixels) \n",
        "\n",
        "\t\t\tmax_index = np.argmax(predictions[0])\n",
        "\t\t\t\n",
        "\t\t\t# copy the image in overlay\n",
        "\t\t\toverlay = img.copy()\n",
        "   \n",
        "      # higher value of opacity, the background will be of lower color\n",
        "\t\t\topacity = 0.2\n",
        "\n",
        "\t\t\tcv2.rectangle(img,(x+w+10,y-25),(x+w+150,y+115),(64,64,64),cv2.FILLED)\n",
        "   \n",
        "\t\t\tcv2.addWeighted(overlay, opacity, img, 1 - opacity, 0, img)\n",
        "\t\t\t\n",
        "\t\t\t# face and expressions are connected\n",
        "\t\t\tcv2.line(img,(int((x+x+w)/2),y+15),(x+w,y-20),(255,255,255),1)\n",
        "   \n",
        "\t\t\tcv2.line(img,(x+w,y-20),(x+w+10,y-20),(255,255,255),1)\n",
        "\t\t\t\n",
        "\t\t\texpression = \"\"\n",
        "\t\t\tfor i in range(len(predictions[0])):\n",
        "     \n",
        "\t\t\t\texpression = \"%s %s%s\" % (expressions[i], round(predictions[0][i]*100, 2), '%')\n",
        "\t\t\t\t\n",
        "\t\t\t\t\"\"\"if i != max_index:\n",
        "\t\t\t\t\tcolor = (255,0,0)\"\"\"\n",
        "\t\t\t\t\t\n",
        "\t\t\t\tcolor = (0,255,255)\n",
        "        \n",
        "\t\t\t\tcv2.putText(img, expression, (int(x+w+15), int(y-12+i*20)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "\t\n",
        "\tcv2_imshow(img)\n",
        "\t\n",
        "\tframe = frame + 1\n",
        "\t\n",
        "\tif frame > 227:\n",
        "\t\tbreak\n",
        "\t\n",
        "\tif cv2.waitKey(1) & 0xFF == ord('q'): #press q to quit\n",
        "\t\tbreak\n",
        "\n",
        "# Close the capturing device\n",
        "video.release()\n",
        "\n",
        "# Close all windows\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}